{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Fashion Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to load a pretrained model and perform test on Fashion Dataset. The model is trained on 40K images. It achieves >70% mAP when test on 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"fashion\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 13  # background + 13 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 10\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Raw dataset related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import lmdb\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotoData(object):\n",
    "    def __init__(self, path):\n",
    "        self.env = lmdb.open(\n",
    "            path, map_size=2**36, readonly=True, lock=False\n",
    "        )\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with self.env.begin() as t:\n",
    "            with t.cursor() as c:\n",
    "                for key, value in c:\n",
    "                    yield key, value\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        key = str(index).encode('ascii')\n",
    "        with self.env.begin() as t:\n",
    "            data = t.get(key)\n",
    "        if not data:\n",
    "            return None\n",
    "        with io.BytesIO(data) as f:\n",
    "            image = Image.open(f)\n",
    "            image.load()\n",
    "            return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.env.stat()['entries']\n",
    "\n",
    "photo_data = PhotoData(r'..'+os.path.sep+'..'+os.path.sep+'..'+os.path.sep+'photos.lmdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.07s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "json_file = r'..' + os.path.sep + '..' + os.path.sep + '..' + os.path.sep + 'modanet2018_instances_train.json'\n",
    "d = json.load(open(json_file))\n",
    "coco=COCO(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion Dataset Class\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "class FashionDataset(utils.Dataset):\n",
    "\n",
    "    def load_fashion(self, count=5, start=0, class_ids=None):\n",
    "        json_file = r'..' + os.path.sep + '..' + os.path.sep + '..' + os.path.sep + 'modanet2018_instances_train.json'\n",
    "        d = json.load(open(json_file))\n",
    "        coco=COCO(json_file)\n",
    "        \n",
    "        if not class_ids:\n",
    "            class_ids = sorted(coco.getCatIds())\n",
    "        \n",
    "        if class_ids:\n",
    "            all_ids = []\n",
    "            for id in class_ids:\n",
    "                all_ids.extend(list(coco.getImgIds(catIds=[id])))\n",
    "            # Remove duplicates\n",
    "            all_ids = list(set(all_ids))\n",
    "        else:\n",
    "            # All images\n",
    "            all_ids = list(coco.imgs.keys())\n",
    "        random.seed(3)\n",
    "        random.shuffle(all_ids)\n",
    "        \n",
    "        all_class_ids = sorted(coco.getCatIds())\n",
    "        for i in all_class_ids:\n",
    "            print('{}:{}'.format(i, coco.loadCats(i)[0]['name']), end='|')\n",
    "            self.add_class(\"fashion\", i, coco.loadCats(i)[0]['name'])\n",
    "            \n",
    "              \n",
    "        image_ids = []\n",
    "        for c in range(count):\n",
    "            image_ids.append(all_ids[c+start])\n",
    "            \n",
    "        # Add images\n",
    "        for i in image_ids:\n",
    "            self.add_image(\n",
    "                \"fashion\", image_id=i,\n",
    "                path=None,\n",
    "                width=coco.imgs[i][\"width\"],\n",
    "                height=coco.imgs[i][\"height\"],\n",
    "                annotations=coco.loadAnns(coco.getAnnIds(\n",
    "                    imgIds=[i], catIds=class_ids, iscrowd=None)))\n",
    "        return image_ids\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        imgId = self.image_info[image_id]['id']\n",
    "        image = photo_data[imgId]\n",
    "        out = np.array(image.getdata()).astype(np.int32).reshape((image.size[1], image.size[0], 3))\n",
    "        return out\n",
    "            \n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Load instance masks for the given image.\n",
    "\n",
    "        Different datasets use different ways to store masks. This\n",
    "        function converts the different mask format to one format\n",
    "        in the form of a bitmap [height, width, instances].\n",
    "\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a COCO image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        annotations = self.image_info[image_id][\"annotations\"]\n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for annotation in annotations:\n",
    "            class_id = annotation['category_id']\n",
    "            if class_id:\n",
    "                m = self.annToMask(annotation, image_info[\"height\"],\n",
    "                                   image_info[\"width\"])\n",
    "                # Some objects are so small that they're less than 1 pixel area\n",
    "                # and end up rounded out. Skip those objects.\n",
    "                if m.max() < 1:\n",
    "                    continue\n",
    "                # Is it a crowd? If so, use a negative class ID.\n",
    "                if annotation['iscrowd']:\n",
    "                    # Use negative class ID for crowds\n",
    "                    class_id *= -1\n",
    "                    # For crowd masks, annToMask() sometimes returns a mask\n",
    "                    # smaller than the given dimensions. If so, resize it.\n",
    "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
    "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
    "                instance_masks.append(m)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        # Pack instance masks into an array\n",
    "        if class_ids:\n",
    "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            return mask, class_ids\n",
    "        else:\n",
    "            # Call super class to return an empty mask\n",
    "            return super(FashionDataset, self).load_mask(image_id)\n",
    "        \n",
    "\n",
    "    def annToRLE(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        segm = ann['segmentation']\n",
    "        if isinstance(segm, list):\n",
    "            # polygon -- a single object might consist of multiple parts\n",
    "            # we merge all parts into one mask rle code\n",
    "            rles = maskUtils.frPyObjects(segm, height, width)\n",
    "            rle = maskUtils.merge(rles)\n",
    "        elif isinstance(segm['counts'], list):\n",
    "            # uncompressed RLE\n",
    "            rle = maskUtils.frPyObjects(segm, height, width)\n",
    "        else:\n",
    "            # rle\n",
    "            rle = ann['segmentation']\n",
    "        return rle\n",
    "\n",
    "    def annToMask(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        rle = self.annToRLE(ann, height, width)\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  C:\\Training\\DeepLearningGit\\virtualFitting\\mask_rcnn\\logs\\fashion20190607T0740\\mask_rcnn_fashion_0200.h5\n",
      "Re-starting from epoch 200\n"
     ]
    }
   ],
   "source": [
    "subset = sorted(coco.getCatIds(catNms=['bag', 'belt', 'outer', 'dress', 'pants', 'top', 'shorts', 'skirt', 'scarf/tie']))\n",
    "test_config = TestConfig()\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=test_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 100 Images\n",
    "We trained our model with previous 40K, so we perform test use images after 40K.\n",
    "## Test Type\n",
    "We test following types:\n",
    "\n",
    "'bag', 'belt', 'outer', 'dress', 'pants', 'top', 'shorts', 'skirt', 'scarf/tie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.93s)\n",
      "creating index...\n",
      "index created!\n",
      "1:bag|2:belt|3:boots|4:footwear|5:outer|6:dress|7:sunglasses|8:pants|9:top|10:shorts|11:skirt|12:headwear|13:scarf/tie|"
     ]
    }
   ],
   "source": [
    "test_count = 100\n",
    "dataset_test = FashionDataset()\n",
    "ids = dataset_test.load_fashion(test_count, start=40001, class_ids=subset)\n",
    "dataset_test.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 0: id:676316, AP:1.0\n",
      "image 1: id:167621, AP:0.6666666865348816\n",
      "image 2: id:414221, AP:0.6666666666666666\n",
      "image 3: id:1014421, AP:0.75\n",
      "image 4: id:487607, AP:1.0\n",
      "image 5: id:128216, AP:1.0\n",
      "image 6: id:945606, AP:0.2\n",
      "image 7: id:129080, AP:1.0\n",
      "image 8: id:1073690, AP:0.7\n",
      "image 9: id:117858, AP:1.0\n",
      "image 10: id:439964, AP:0.6666666865348816\n",
      "image 11: id:146537, AP:0.6666666666666666\n",
      "image 12: id:183073, AP:1.0\n",
      "image 13: id:174754, AP:0.5\n",
      "image 14: id:108687, AP:1.0\n",
      "image 15: id:489028, AP:0.3333333432674408\n",
      "image 16: id:1000769, AP:1.0\n",
      "image 17: id:393374, AP:1.0\n",
      "image 18: id:432179, AP:1.0\n",
      "image 19: id:326254, AP:1.0\n",
      "image 20: id:645612, AP:0.3333333432674408\n",
      "image 21: id:163323, AP:0.6666666865348816\n",
      "image 22: id:636217, AP:0.25\n",
      "image 23: id:150198, AP:1.0\n",
      "image 24: id:183923, AP:0.5\n",
      "image 25: id:531848, AP:0.6666666865348816\n",
      "image 26: id:279616, AP:0.3333333432674408\n",
      "image 27: id:724837, AP:1.0\n",
      "image 28: id:627103, AP:0.375\n",
      "image 29: id:480365, AP:0.5\n",
      "image 30: id:109901, AP:0.95\n",
      "image 31: id:390955, AP:0.75\n",
      "image 32: id:449895, AP:0.75\n",
      "image 33: id:1018950, AP:1.0\n",
      "image 34: id:148114, AP:0.6666666865348816\n",
      "image 35: id:150326, AP:0.0\n",
      "image 36: id:820159, AP:0.444444457689921\n",
      "image 37: id:188098, AP:1.0\n",
      "image 38: id:1025497, AP:1.0\n",
      "image 39: id:387212, AP:1.0\n",
      "image 40: id:503810, AP:0.3333333432674408\n",
      "image 41: id:447922, AP:0.42857144134385244\n",
      "image 42: id:498878, AP:1.0\n",
      "image 43: id:370252, AP:0.6666666865348816\n",
      "image 44: id:105765, AP:1.0\n",
      "image 45: id:1078059, AP:0.5\n",
      "image 46: id:425656, AP:0.75\n",
      "image 47: id:409554, AP:1.0\n",
      "image 48: id:805431, AP:0.75\n",
      "image 49: id:151271, AP:0.5\n",
      "image 50: id:423175, AP:0.6041666666666666\n",
      "image 51: id:537482, AP:0.6666666865348816\n",
      "image 52: id:412833, AP:0.75\n",
      "image 53: id:374652, AP:0.5\n",
      "image 54: id:257478, AP:0.5500000193715096\n",
      "image 55: id:380759, AP:0.9523809552192688\n",
      "image 56: id:155539, AP:1.0\n",
      "image 57: id:1032835, AP:1.0\n",
      "image 58: id:148353, AP:0.6666666865348816\n",
      "image 59: id:392995, AP:1.0\n",
      "image 60: id:1044162, AP:1.0\n",
      "image 61: id:967259, AP:0.4666666805744171\n",
      "image 62: id:144134, AP:1.0\n",
      "image 63: id:539827, AP:0.25\n",
      "image 64: id:175840, AP:0.41666666666666663\n",
      "image 65: id:802791, AP:0.41666666666666663\n",
      "image 66: id:662533, AP:0.5\n",
      "image 67: id:1012492, AP:1.0\n",
      "image 68: id:522039, AP:0.11111111442248026\n",
      "image 69: id:295113, AP:0.75\n",
      "image 70: id:1006277, AP:1.0\n",
      "image 71: id:188072, AP:1.0\n",
      "image 72: id:1045875, AP:1.0\n",
      "image 73: id:687557, AP:0.5555555721124013\n",
      "image 74: id:130032, AP:1.0\n",
      "image 75: id:450511, AP:1.0\n",
      "image 76: id:343267, AP:0.75\n",
      "image 77: id:1098257, AP:0.95\n",
      "image 78: id:1046253, AP:0.3333333432674408\n",
      "image 79: id:719484, AP:0.95\n",
      "image 80: id:885769, AP:0.25\n",
      "image 81: id:162930, AP:0.75\n",
      "image 82: id:906290, AP:1.0\n",
      "image 83: id:175451, AP:1.0\n",
      "image 84: id:466416, AP:1.0\n",
      "image 85: id:243612, AP:0.4666666805744171\n",
      "image 86: id:340259, AP:1.0\n",
      "image 87: id:703071, AP:0.5000000149011612\n",
      "image 88: id:1094417, AP:0.8333333358168602\n",
      "image 89: id:778480, AP:0.6666666865348816\n",
      "image 90: id:165963, AP:0.5\n",
      "image 91: id:473826, AP:1.0\n",
      "image 92: id:820195, AP:1.0\n",
      "image 93: id:548896, AP:0.9166666716337204\n",
      "image 94: id:181773, AP:0.5\n",
      "image 95: id:378742, AP:0.9166666716337204\n",
      "image 96: id:1062594, AP:1.0\n",
      "image 97: id:722030, AP:0.75\n",
      "image 98: id:99324, AP:0.3333333432674408\n",
      "image 99: id:679401, AP:0.5\n",
      "mAP:  0.7328789718704565\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 100 images. Increase for better accuracy.\n",
    "image_ids = range(100)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_test, test_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, test_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    print('image '+str(image_id)+\": id:\"+str(ids[image_id])+ ', AP:' + str(AP) )        \n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
